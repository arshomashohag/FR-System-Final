{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data pre-processing"},{"metadata":{"_kg_hide-input":false,"_uuid":"875b42ec5baee5274279d8a7b7a72159f3a586de","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os # accessing directory structure\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"7c96d605282a65cebf83737bbf0a3386c5c3f19e","trusted":true},"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/fashion-dataset/\"\nprint(os.listdir(DATASET_PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_meta = pd.read_csv(DATASET_PATH+'styles.csv',error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_meta.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_meta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_meta['image'] = image_meta.apply(lambda row: str(row['id']) + \".jpg\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_meta.masterCategory.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_meta = image_meta.groupby('masterCategory').filter(lambda x: len(x) > 2000);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_image_meta = train_image_meta.groupby('masterCategory').head(2400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_image_meta.masterCategory.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_meta, test_image_meta = train_test_split(train_image_meta, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_meta, val_image_meta = train_test_split(train_image_meta, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_meta.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"delete_row = train_image_meta[train_image_meta[\"id\"]==12347].index\ntrain_image_meta = train_image_meta.drop(delete_row)\ndelete_row = train_image_meta[train_image_meta[\"id\"]==39403].index\ntrain_image_meta = train_image_meta.drop(delete_row)\ndelete_row = train_image_meta[train_image_meta[\"id\"]==39410].index\ntrain_image_meta = train_image_meta.drop(delete_row)\ndelete_row = train_image_meta[train_image_meta[\"id\"]==39425].index\ntrain_image_meta = train_image_meta.drop(delete_row)\ndelete_row = train_image_meta[train_image_meta[\"id\"]==39401].index\ntrain_image_meta = train_image_meta.drop(delete_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_WIDTH=100\nIMG_HEIGHT=75\nIMG_DIM = (IMG_HEIGHT,IMG_WIDTH)\nIMG_PATH = DATASET_PATH + 'images/'\nval_img = [img_to_array(load_img(IMG_PATH+img.image, target_size=IMG_DIM)) for i, img in val_image_meta.iterrows()]\nval_img = np.array(val_img)\nval_labels = [img.masterCategory for i, img in val_image_meta.iterrows()]\ntrain_imgs = [img_to_array(load_img(IMG_PATH+img.image, target_size=IMG_DIM)) for i, img in train_image_meta.iterrows()]\ntrain_imgs = np.array(train_imgs)\ntrain_labels = [img.masterCategory for i, img in train_image_meta.iterrows()]\ntest_imgs = [img_to_array(load_img(IMG_PATH+img.image, target_size=IMG_DIM)) for i, img in test_image_meta.iterrows()]\ntest_imgs = np.array(test_imgs)\ntest_labels = [img.masterCategory for i, img in test_image_meta.iterrows()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_imgs_class = [img.masterCategory for i, img in test_image_meta.iterrows()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_imgs_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode text category labels \nfrom sklearn.preprocessing import LabelEncoder \n \nle = LabelEncoder() \nle.fit(train_labels) \ntrain_labels_enc = le.transform(train_labels)\nle.fit(val_labels)\nval_labels_enc = le.transform(val_labels)\nle.fit(test_labels)\ntest_labels_enc = le.transform(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name_labels = [(train_labels[i], train_labels_enc[i]) for i in range(0, len(train_labels_enc))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ninput_shape = (IMG_HEIGHT,IMG_WIDTH)\nNUM_CLASSES = len(Counter(train_labels_enc).keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_CLASSES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_enc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\ntrain_labels_enc = to_categorical(train_labels_enc)\nval_labels_enc = to_categorical(val_labels_enc)\ntest_labels_enc = to_categorical(test_labels_enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_enc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_labels_enc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels_enc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rotation_range=30,\n                                   zoom_range=0.15,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.15,\n                                   horizontal_flip=True,\n                                   fill_mode=\"nearest\")\nval_datagen = ImageDataGenerator()\ntest_datagen = ImageDataGenerator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = train_datagen.flow(train_imgs, \n                                     train_labels_enc,\n                                     batch_size=25, \n                                     shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_generator = val_datagen.flow(val_img, \n                                     val_labels_enc,\n                                     batch_size=25, \n                                     shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator = test_datagen.flow(test_imgs, \n                                test_labels_enc,\n                                batch_size=1, \n                                shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.xception import Xception\nfrom keras.models import Model\nimport keras\nxception = Xception(include_top=False, weights='imagenet', input_shape=(IMG_HEIGHT,IMG_WIDTH,3))\n# output = restnet.layers[-1].output\n# output = keras.layers.Flatten()(output)\n# restnet = Model(restnet.input, output=output)\n# xception layers number:132\nfor layer in xception.layers:\n    layer.trainable = False\nxception.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, GlobalAveragePooling2D, Flatten, Dense, Dropout, InputLayer\n# from keras.models import Sequential\nfrom keras import optimizers\n# model_finetuned = Sequential()\n# model_finetuned.add(restnet)\n# model_finetuned.add(Dense(512, activation='relu'))\n# model_finetuned.add(Dropout(0.3))\n# model_finetuned.add(Dense(NUM_CLASSES, activation='softmax'))\n# model_finetuned.compile(loss='categorical_crossentropy',\n#               optimizer=optimizers.RMSprop(lr=1e-5),\n#               metrics=['accuracy'])\n# model_finetuned.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = xception\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\n# and a logistic layer -- let's say we have 7 classes\npredictions = Dense(NUM_CLASSES, activation='softmax')(x) \nmodel_finetuned = Model(inputs=base_model.input, outputs=predictions)\nmodel_finetuned.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=0.00001),\n              metrics=['accuracy'])\nmodel_finetuned.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_1 = model_finetuned.fit_generator(train_generator, \n                                          steps_per_epoch=None, \n                                          epochs=50, \n                                          verbose=1, \n                                          use_multiprocessing=True,\n                                          validation_data = validation_generator,\n                                          workers=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_1.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = history_1.history['accuracy']\nval_acc = history_1.history['val_accuracy']\n\nloss = history_1.history['loss']\nval_loss = history_1.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='center right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,15.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(val_loss)/len(val_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_samples = len(test_imgs_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model_finetuned.predict_generator(test_generator,steps=nb_samples, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class_indices=np.argmax(predict,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predicted_class_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_imgs_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels_enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_class_indices=np.argmax(test_labels_enc,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = 0\nfor i in range(len(test_class_indices)):\n    if predicted_class_indices[i] == test_class_indices[i]:\n        correct = correct + 1\nprint(correct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracy = (correct*100.00)/len(test_class_indices)\nprint(test_accuracy)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}